<div style="font-family: Arial, sans-serif; line-height: 1.5; color: #222;">

<h3>💡 Example Questions</h3>
<ul style="padding-left: 1.5em;">
    <li>What is your knowledge about?</li>
    <li>What are KG-TRACES?</li>
    <li>What is L3Cube-MahaEmotions?</li>
    <li>Explain KG-Traces for beginners with no IT background</li>
    <li>What are the dangers and risks of LLMs?</li>
    <li>Qu’est-ce qu’un grand modèle de langage ? Réponds en français</li>
    <li>Schreibe ein Gedicht über ein LLM auf deutsch</li>
</ul>

<h3>📂 Project Info</h3>
<ul style="padding-left: 1.5em;">

    <li>🔗 <b>GitHub:</b> <a href="https://github.com/ferdinand-koenig/insight-bridge" target="_blank">ferdinand-koenig/insight-bridge</a> |
        <b>LinkedIn:</b> <a href="https://www.linkedin.com/in/ferdinand-koenig" target="_blank">ferdinand-koenig</a>
    </li>
    <li>✉️ <b>Email:</b> {obfuscated_email}</li>

    <li>⚙️ <b>An MLOps Project:</b> End-to-end pipeline for LLM inference, from data ingestion to web delivery.
        <br>Includes automatic download of arXiv preprints, preprocessing, embedding with SentenceTransformers, FAISS indexing, and semantic Q&A.
    </li>

    <li>🛠️ <b>Tech Stack:</b>
        LangChain, HuggingFace Transformers, SentenceTransformers, FAISS, llama.cpp (quantized LLMs),
        Dockerized Deployment, Cloud Orchestration (Hetzner), Python Logging (monitoring),
        Gradio Web UI, WebPush Notifications, Progressive Web App (PWA).
        <br><i>Focus: scalable LLM inference, efficient CPU deployment, and production-ready pipelines.</i>
    </li>

    <li>👉 <b>Do you like this project?</b> Want a similar WebApp for your team or research group? Reach out to me.</li>

    <li>🤖 <b>Model:</b> <i>Mistral-Nemo-12B-Instruct-2407-Q4_K_M.gguf</i> (12B parameters, instruction-tuned, quantized for efficiency).
        <br>💡 <i>Purpose:</i> Independent semantic Q&A over curated AI preprints (not a chatbot).
        <br>💡 <i>Comparison:</i> Much smaller than proprietary models (e.g. GPT-4) or large open models (e.g. Llama-3.1-405B).
    </li>

    <li>💾 <b>Resource Efficient:</b> Runs entirely on CPU.
        <br>~25× cheaper than GPU inference (Hetzner Cloud).
        ⏱️ <i>Latency:</i> ~10s if cached, 2–4 min for new queries.
    </li>

    <li>🔒 <b>High Privacy:</b> Local inference or via secure SSH tunnels; no data sent to third parties.</li>

    <li>⚠️ <b>Limitations:</b> Smaller models → less accurate than trillion-parameter LLMs. Limited document set.</li>

    <li>📲 <b>Installable:</b> Progressive Web App → can be added to devices like a native app.</li>

    <li>📊 <b>Architecture</b><br>
        <i>For a full architecture description and implementation details, see the
        <a href="https://github.com/ferdinand-koenig/insight-bridge" target="_blank">GitHub README</a>.</i>
    </li>
</ul>

<h3>⚠️ Disclaimer</h3>
<p>
This project is a personal demo for portfolio purposes. All input data is anonymous.<br>
Outputs of the LLM may contain inaccuracies and should not be used for critical decisions.<br>
This application uses cookies for notifications only. A pseudonymous identifier is transferred only if notifications are accepted.
</p>

</div>
