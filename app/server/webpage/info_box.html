<div style="font-family: Arial, sans-serif; line-height: 1.5; color: #222;">

<h3>ğŸ’¡ Example Questions</h3>
<ul style="padding-left: 1.5em;">
    <li>What is your knowledge about?</li>
    <li>What are KG-TRACES?</li>
    <li>What is L3Cube-MahaEmotions?</li>
    <li>Explain KG-Traces for beginners with no IT background</li>
    <li>What are the dangers and risks of LLMs?</li>
    <li>Quâ€™est-ce quâ€™un grand modÃ¨le de langage ? RÃ©ponds en franÃ§ais</li>
    <li>Schreibe ein Gedicht Ã¼ber ein LLM auf deutsch</li>
</ul>

<h3>ğŸ“‚ Project Info</h3>
<ul style="padding-left: 1.5em;">

    <li>ğŸ”— <b>GitHub:</b> <a href="https://github.com/ferdinand-koenig/insight-bridge" target="_blank">ferdinand-koenig/insight-bridge</a> |
        <b>LinkedIn:</b> <a href="https://www.linkedin.com/in/ferdinand-koenig" target="_blank">ferdinand-koenig</a>
    </li>
    <li>âœ‰ï¸ <b>Email:</b> {obfuscated_email}</li>

    <li>âš™ï¸ <b>An MLOps Project:</b> End-to-end pipeline for LLM inference, from data ingestion to web delivery.
        <br>Includes automatic download of arXiv preprints, preprocessing, embedding with SentenceTransformers, FAISS indexing, and semantic Q&A.
    </li>

    <li>ğŸ› ï¸ <b>Tech Stack:</b>
        LangChain, HuggingFace Transformers, SentenceTransformers, FAISS, llama.cpp (quantized LLMs),
        Dockerized Deployment, Cloud Orchestration (Hetzner), Python Logging (monitoring),
        Gradio Web UI, WebPush Notifications, Progressive Web App (PWA).
        <br><i>Focus: scalable LLM inference, efficient CPU deployment, and production-ready pipelines.</i>
    </li>

    <li>ğŸ‘‰ <b>Do you like this project?</b> Want a similar WebApp for your team or research group? Reach out to me.</li>

    <li>ğŸ¤– <b>Model:</b> <i>Mistral-Nemo-12B-Instruct-2407-Q4_K_M.gguf</i> (12B parameters, instruction-tuned, quantized for efficiency).
        <br>ğŸ’¡ <i>Purpose:</i> Independent semantic Q&A over curated AI preprints (not a chatbot).
        <br>ğŸ’¡ <i>Comparison:</i> Much smaller than proprietary models (e.g. GPT-4) or large open models (e.g. Llama-3.1-405B).
    </li>

    <li>ğŸ’¾ <b>Resource Efficient:</b> Runs entirely on CPU.
        <br>~25Ã— cheaper than GPU inference (Hetzner Cloud).
        â±ï¸ <i>Latency:</i> ~10s if cached, 2â€“4 min for new queries.
    </li>

    <li>ğŸ”’ <b>High Privacy:</b> Local inference or via secure SSH tunnels; no data sent to third parties.</li>

    <li>âš ï¸ <b>Limitations:</b> Smaller models â†’ less accurate than trillion-parameter LLMs. Limited document set.</li>

    <li>ğŸ“² <b>Installable:</b> Progressive Web App â†’ can be added to devices like a native app.</li>

    <li>ğŸ“Š <b>Architecture</b><br>
        <i>For a full architecture description and implementation details, see the
        <a href="https://github.com/ferdinand-koenig/insight-bridge" target="_blank">GitHub README</a>.</i>
    </li>
</ul>

<h3>âš ï¸ Disclaimer</h3>
<p>
This project is a personal demo for portfolio purposes. All input data is anonymous.<br>
Outputs of the LLM may contain inaccuracies and should not be used for critical decisions.<br>
This application uses cookies for notifications only. A pseudonymous identifier is transferred only if notifications are accepted.
</p>

</div>
