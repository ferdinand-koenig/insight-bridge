embedding_model_name: "sentence-transformers/msmarco-distilbert-base-v4"  # Sentence embedding model for FAISS search
top_k: 3  # How many most relevant chunks to retrieve from FAISS

# Path to folder with raw documents before embedding
raw_docs_path: "data/raw"  # "/vol/fob-vol5/mi22/koenigfd/data/raw/raw"

arxiv_download:
  categories:
    - "cs.CL"  # ArXiv category for NLP & LLM papers
  year: 2025  # Year filter for ArXiv download
  month: 6    # Month filter for ArXiv download

# FAISS paths. Used for creation and use (Read and Write)
FAISS:
  # FAISS binary index location
  index_path: "data/faiss_index"  #  "/vol/fob-vol5/mi22/koenigfd/data/faiss_index"
  # Pickled metadata (docstore + ID mapping)
  metadata_path: "data/faiss_metadata.pkl"  # "/vol/fob-vol5/mi22/koenigfd/data/faiss_metadata.pkl"

RAG:
  chunk_size: 1000       # Number of characters per text chunk
  chunk_overlap: 150     # Overlap between chunks to preserve context
  batch_size: 4096 * 2   # Adjust to memory limits; Batch size for embedding computation

gradio_server:
  host: "0.0.0.0"  # Use 127.0.0.1 for local-only, 0.0.0.0 for public
  port: 8080      # Gradio web app port
  use_mock_answer: false
  ssl_cert_path: /insight-bridge/certs/fullchain.pem
  ssl_key_path: /insight-bridge/certs/privkey.pem
  hit_delay: 10

worker_server:
  host: "0.0.0.0"  # Use 127.0.0.1 for private localhost, else 0.0.0.0 (always when in docker)
  port: 8000

backend_pool:
  mode: "local"  # "local" or "hetzner"
  max_backends: 5
  idle_buffer: 360  # seconds
  health_timeout: 600
  inference_timeout: 600  #300

hetzner:
  snapshot_name: "docker-ce"
  ssh_key_name: "llm-hetzner-main-server"
  ssh_key_path: "~/.ssh/llm-hetzner-main-server"
  server_type: ["CX52", "CX42", "CPX41", "CX32", "CPX31"]
  private_network_name: "network-1"

llm:
  model_name: "/model/mistral-7b-instruct-v0.2.Q4_K_M.gguf"  # Path or HuggingFace model ID
  # other popular choices:
    #  mistral-7b-instruct-v0.2.Q4_K_M.gguf
    #  phi-2.Q4_K_M.gguf
    #  phi-3.unsloth.Q4_K_M.gguf
    #  qwen2-1.5b-instruct-q4_k_m.gguf
    #  tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf
  # "Qwen/Qwen2.5-0.5B-Instruct",  #  "Qwen/Qwen3-0.6B",  # "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
  max_length: 256 #384    # Max output tokens for conciseness and efficiency
  temperature: 0.1        # Lower = more deterministic answers
  do_sample: false        # false = deterministic decoding
  no_repeat_ngram_size: 3 # Prevent repeated phrases
  num_beams: 1            # Beam search width (1 = greedy search)
  stop:
    - "\n"                # Stop generation on newline
  context_length: 4096    # Maximum context tokens the model can process
  # add n_threads if you want to specify a number of CPU threads to be used. Dont specify to use all
#  n_threads: 6

logging:
  level: DEBUG
